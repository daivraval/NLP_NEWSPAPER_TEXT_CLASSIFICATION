{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40fdd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.7.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.2)\n",
      "Requirement already satisfied: flask in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.15.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from flask) (3.0.2)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from flask) (8.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: colorama in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deiv\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Deiv\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn joblib flask nltk scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57e9c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "samples: 4849, classes: 5\n",
      "Training pipeline...\n",
      "Predicting test set...\n",
      "Accuracy: 0.9124\n",
      "Precision (weighted): 0.9167\n",
      "Recall (weighted): 0.9124\n",
      "F1 (weighted): 0.9125\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.91      0.91      0.91       195\n",
      "          misc.forsale       0.96      0.88      0.92       195\n",
      "    rec.sport.baseball       0.84      0.99      0.91       199\n",
      "soc.religion.christian       0.96      0.87      0.92       199\n",
      "    talk.politics.guns       0.92      0.91      0.91       182\n",
      "\n",
      "              accuracy                           0.91       970\n",
      "             macro avg       0.92      0.91      0.91       970\n",
      "          weighted avg       0.92      0.91      0.91       970\n",
      "\n",
      "Saved pipeline to news_tfidf_lr.joblib\n"
     ]
    }
   ],
   "source": [
    "# news_tfidf_classifier.py\n",
    "\"\"\"\n",
    "TF-IDF -> classifier for selected 20Newsgroups categories.\n",
    "- Uses TfidfVectorizer with built-in stopword removal for speed.\n",
    "- Uses LogisticRegression (fast & well-suited to sparse text).\n",
    "- Prints accuracy, precision/recall/f1, classification report.\n",
    "- Saves pipeline to news_tfidf_lr.joblib\n",
    "\"\"\"\n",
    "\n",
    "from html import parser\n",
    "import joblib\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import argparse\n",
    "\n",
    "def minimal_clean(text):\n",
    "    # optional mild cleanup (keep it fast)\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # remove long runs of whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_data(categories):\n",
    "    data = fetch_20newsgroups(categories=categories, subset='all',\n",
    "                              remove=('headers','footers','quotes'))\n",
    "    X = [minimal_clean(t) for t in data.data]\n",
    "    y = data.target\n",
    "    target_names = data.target_names\n",
    "    return X, y, target_names\n",
    "\n",
    "def build_pipeline():\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2),\n",
    "                           max_df=0.9,\n",
    "                           min_df=3,\n",
    "                           sublinear_tf=True,\n",
    "                           stop_words='english')  # use built-in stopwords\n",
    "    clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, random_state=42)\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", vect),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    return pipe\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--test_size\", type=float, default=0.2)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball',\n",
    "                  'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    X, y, target_names = load_data(categories)\n",
    "    print(f\"samples: {len(X)}, classes: {len(target_names)}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=args.test_size,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    pipe = build_pipeline()\n",
    "    print(\"Training pipeline...\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Predicting test set...\")\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision (weighted): {prec:.4f}\")\n",
    "    print(f\"Recall (weighted): {rec:.4f}\")\n",
    "    print(f\"F1 (weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification report:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "    # Save pipeline\n",
    "    model_path = \"news_tfidf_lr.joblib\"\n",
    "    joblib.dump(pipe, model_path)\n",
    "    print(f\"Saved pipeline to {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cdae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: rtx 4050 laptops are good\n",
      " → Predicted category: rec.sport.baseball\n",
      "\n",
      "Text: I bought a laptop on sale last week.\n",
      " → Predicted category: misc.forsale\n",
      "\n",
      "Text: The baseball team won their match yesterday.\n",
      " → Predicted category: rec.sport.baseball\n",
      "\n",
      "Text: God is love and faith is important in life.\n",
      " → Predicted category: soc.religion.christian\n",
      "\n",
      "Text: The debate on gun laws is heating up again in politics.\n",
      " → Predicted category: talk.politics.guns\n",
      "\n",
      "Text: He bought a new computer with a powerful GPU.\n",
      " → Predicted category: misc.forsale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load saved model\n",
    "model = joblib.load(\"news_tfidf_lr.joblib\")\n",
    "\n",
    "# Same categories as training\n",
    "categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball',\n",
    "              'soc.religion.christian', 'talk.politics.guns']\n",
    "target_names = fetch_20newsgroups(categories=categories, subset='train').target_names\n",
    "\n",
    "# Try some custom texts\n",
    "examples = [\n",
    "    \"rtx 4050 laptops are good\",\n",
    "    \"I bought a laptop on sale last week.\",\n",
    "    \"The baseball team won their match yesterday.\",\n",
    "    \"God is love and faith is important in life.\",\n",
    "    \"The debate on gun laws is heating up again in politics.\",\n",
    "    \"He bought a new computer with a powerful GPU.\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    pred = model.predict([text])[0]\n",
    "    print(f\"Text: {text}\\n → Predicted category: {target_names[pred]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af35954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "text = \"3050 is good\"\n",
    "pred = model.predict([text])[0]\n",
    "print(\"Predicted category:\", target_names[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32a090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f9923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "samples: 4849, classes: 5\n",
      "Building TF-IDF...\n",
      "TF-IDF shapes: (3879, 26918) (970, 26918)\n",
      "Running TruncatedSVD (n_components=100)...\n",
      "LSA shapes: (3879, 100) (970, 100)\n",
      "Combined shapes: (3879, 27018) (970, 27018)\n",
      "Training classifier on combined features...\n",
      "Predicting test set...\n",
      "Accuracy: 0.9010\n",
      "Precision (weighted): 0.9041\n",
      "Recall (weighted): 0.9010\n",
      "F1 (weighted): 0.9010\n",
      "\n",
      "Classification report:\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         comp.graphics       0.90      0.88      0.89       195\n",
      "          misc.forsale       0.92      0.89      0.90       195\n",
      "    rec.sport.baseball       0.84      0.98      0.90       199\n",
      "soc.religion.christian       0.95      0.87      0.91       199\n",
      "    talk.politics.guns       0.91      0.88      0.90       182\n",
      "\n",
      "              accuracy                           0.90       970\n",
      "             macro avg       0.90      0.90      0.90       970\n",
      "          weighted avg       0.90      0.90      0.90       970\n",
      "\n",
      "Saved hybrid model bundle to news_tfidf_lsa_lr.joblib\n"
     ]
    }
   ],
   "source": [
    "# hybripyd_tfidf_lsa.\n",
    "\"\"\"\n",
    "Hybrid TF-IDF + LSA -> Logistic Regression classifier.\n",
    "\n",
    "Saves model components to news_tfidf_lsa_lr.joblib.\n",
    "\n",
    "Usage:\n",
    "    python hybrid_tfidf_lsa.py --n_components 100 --test_size 0.2\n",
    "\"\"\"\n",
    "import argparse\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def minimal_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_data(categories):\n",
    "    data = fetch_20newsgroups(categories=categories, subset='all', remove=('headers','footers','quotes'))\n",
    "    X = [minimal_clean(t) for t in data.data]\n",
    "    y = data.target\n",
    "    target_names = data.target_names\n",
    "    return X, y, target_names\n",
    "\n",
    "def build_tfidf(train_texts, ngram_range=(1,2), max_df=0.9, min_df=3):\n",
    "    vect = TfidfVectorizer(ngram_range=ngram_range,\n",
    "                           max_df=max_df, min_df=min_df,\n",
    "                           sublinear_tf=True, stop_words='english')\n",
    "    X_tfidf = vect.fit_transform(train_texts)\n",
    "    return vect, X_tfidf\n",
    "\n",
    "def combine_sparse_dense(X_sparse, X_dense):\n",
    "    \"\"\"Concatenate sparse matrix with dense array (converted to sparse).\"\"\"\n",
    "    return sparse.hstack([X_sparse, sparse.csr_matrix(X_dense)], format='csr')\n",
    "\n",
    "def evaluate(y_true, y_pred, target_names):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision (weighted): {prec:.4f}\")\n",
    "    print(f\"Recall (weighted): {rec:.4f}\")\n",
    "    print(f\"F1 (weighted): {f1:.4f}\\n\")\n",
    "    print(\"Classification report:\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_components\", type=int, default=100, help=\"TruncatedSVD components (LSA dims)\")\n",
    "    parser.add_argument(\"--test_size\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--compare_baseline\", action=\"store_true\",\n",
    "                        help=\"If set, attempt to load news_tfidf_lr.joblib and compare performance on the same test set\")\n",
    "    # safe for notebooks\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball',\n",
    "                  'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    X, y, target_names = load_data(categories)\n",
    "    print(f\"samples: {len(X)}, classes: {len(target_names)}\")\n",
    "\n",
    "    # split raw texts (we'll fit tfidf on train only)\n",
    "    X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=args.test_size, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # TF-IDF\n",
    "    print(\"Building TF-IDF...\")\n",
    "    tfidf_vect, X_train_tfidf = build_tfidf(X_train_texts)\n",
    "    X_test_tfidf = tfidf_vect.transform(X_test_texts)\n",
    "    print(\"TF-IDF shapes:\", X_train_tfidf.shape, X_test_tfidf.shape)\n",
    "\n",
    "    # LSA via TruncatedSVD on train TF-IDF\n",
    "    print(f\"Running TruncatedSVD (n_components={args.n_components})...\")\n",
    "    svd = TruncatedSVD(n_components=args.n_components, random_state=42)\n",
    "    X_train_lsa = svd.fit_transform(X_train_tfidf)\n",
    "    X_test_lsa = svd.transform(X_test_tfidf)\n",
    "    print(\"LSA shapes:\", X_train_lsa.shape, X_test_lsa.shape)\n",
    "\n",
    "    # Scale dense chunk (important when concatenating)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lsa_scaled = scaler.fit_transform(X_train_lsa)\n",
    "    X_test_lsa_scaled = scaler.transform(X_test_lsa)\n",
    "\n",
    "    # Concatenate sparse TF-IDF + dense LSA (converted to sparse)\n",
    "    X_train_comb = combine_sparse_dense(X_train_tfidf, X_train_lsa_scaled)\n",
    "    X_test_comb  = combine_sparse_dense(X_test_tfidf, X_test_lsa_scaled)\n",
    "    print(\"Combined shapes:\", X_train_comb.shape, X_test_comb.shape)\n",
    "\n",
    "    # Train classifier (LogisticRegression)\n",
    "    clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, random_state=42)\n",
    "    print(\"Training classifier on combined features...\")\n",
    "    clf.fit(X_train_comb, y_train)\n",
    "\n",
    "    # Predict + evaluate\n",
    "    print(\"Predicting test set...\")\n",
    "    y_pred = clf.predict(X_test_comb)\n",
    "    evaluate(y_test, y_pred, target_names)\n",
    "\n",
    "    # Save the needed components so we can load later for inference\n",
    "    model_bundle = {\n",
    "        \"tfidf_vect\": tfidf_vect,\n",
    "        \"svd\": svd,\n",
    "        \"scaler\": scaler,\n",
    "        \"clf\": clf,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    out_path = \"news_tfidf_lsa_lr.joblib\"\n",
    "    joblib.dump(model_bundle, out_path)\n",
    "    print(f\"Saved hybrid model bundle to {out_path}\")\n",
    "\n",
    "    # Optional: compare with baseline model if user asked and baseline exists\n",
    "    if args.compare_baseline:\n",
    "        baseline_path = \"news_tfidf_lr.joblib\"\n",
    "        if os.path.exists(baseline_path):\n",
    "            print(\"\\nLoading baseline pipeline and comparing on the same test set...\")\n",
    "            baseline = joblib.load(baseline_path)\n",
    "            # baseline expects raw text input (pipeline), so pass X_test_texts\n",
    "            base_pred = baseline.predict(X_test_texts)\n",
    "            print(\"Baseline metrics:\")\n",
    "            evaluate(y_test, base_pred, target_names)\n",
    "            print(\"Hybrid model metrics (repeating for convenience):\")\n",
    "            evaluate(y_test, y_pred, target_names)\n",
    "        else:\n",
    "            print(f\"Baseline file {baseline_path} not found. Skip baseline comparison.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0421435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Samples: 4849, classes: 5\n",
      "\n",
      "--- Baseline (TF-IDF -> LR) ---\n",
      "Baseline metrics (acc, prec, rec, f1): (0.9123711340206185, 0.9167256087225616, 0.9123711340206185, 0.9125281263649558)\n",
      "\n",
      "--- Hybrid (n_components=50) ---\n",
      "Hybrid (n=50) metrics (acc, prec, rec, f1): (0.9072164948453608, 0.9101398589692358, 0.9072164948453608, 0.9073412024506444)\n",
      "\n",
      "--- Hybrid (n_components=100) ---\n",
      "Hybrid (n=100) metrics (acc, prec, rec, f1): (0.9010309278350516, 0.9040882038651065, 0.9010309278350516, 0.9010129511678747)\n",
      "\n",
      "--- Hybrid (n_components=200) ---\n",
      "Hybrid (n=200) metrics (acc, prec, rec, f1): (0.9010309278350516, 0.903825314219625, 0.9010309278350516, 0.9011147864027895)\n",
      "\n",
      "\n",
      "=== Comparison table (sorted by accuracy) ===\n",
      "              model  n_components  accuracy  precision  recall     f1\n",
      "  baseline_tfidf_lr           NaN    0.9124     0.9167  0.9124 0.9125\n",
      "hybrid_tfidf_lsa_lr       50.0000    0.9072     0.9101  0.9072 0.9073\n",
      "hybrid_tfidf_lsa_lr      100.0000    0.9010     0.9041  0.9010 0.9010\n",
      "hybrid_tfidf_lsa_lr      200.0000    0.9010     0.9038  0.9010 0.9011\n"
     ]
    }
   ],
   "source": [
    "# compare_models.py\n",
    "\"\"\"\n",
    "Train & compare:\n",
    " - Baseline: TF-IDF -> LogisticRegression\n",
    " - Hybrid:  TF-IDF -> TruncatedSVD -> concat -> LogisticRegression\n",
    "Runs hybrid for multiple n_components and prints a comparison table.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "\n",
    "def minimal_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_data(categories):\n",
    "    data = fetch_20newsgroups(categories=categories, subset='all', remove=('headers','footers','quotes'))\n",
    "    X = [minimal_clean(t) for t in data.data]\n",
    "    y = data.target\n",
    "    target_names = data.target_names\n",
    "    return X, y, target_names\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def baseline_pipeline_train_eval(X_train, X_test, y_train, y_test):\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=3, sublinear_tf=True, stop_words='english')\n",
    "    clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, random_state=42)\n",
    "    # Fit\n",
    "    X_train_tfidf = vect.fit_transform(X_train)\n",
    "    X_test_tfidf = vect.transform(X_test)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    metrics = evaluate_metrics(y_test, y_pred)\n",
    "    bundle = {\"vect\": vect, \"clf\": clf}\n",
    "    return metrics, bundle\n",
    "\n",
    "def hybrid_train_eval(X_train, X_test, y_train, y_test, n_components):\n",
    "    # TF-IDF on train\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=3, sublinear_tf=True, stop_words='english')\n",
    "    X_train_tfidf = vect.fit_transform(X_train)\n",
    "    X_test_tfidf = vect.transform(X_test)\n",
    "\n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    X_train_lsa = svd.fit_transform(X_train_tfidf)\n",
    "    X_test_lsa = svd.transform(X_test_tfidf)\n",
    "\n",
    "    # scale dense chunk\n",
    "    scaler = StandardScaler()\n",
    "    X_train_lsa_s = scaler.fit_transform(X_train_lsa)\n",
    "    X_test_lsa_s = scaler.transform(X_test_lsa)\n",
    "\n",
    "    # combine\n",
    "    X_train_comb = sparse.hstack([X_train_tfidf, sparse.csr_matrix(X_train_lsa_s)], format='csr')\n",
    "    X_test_comb  = sparse.hstack([X_test_tfidf,  sparse.csr_matrix(X_test_lsa_s)], format='csr')\n",
    "\n",
    "    clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, random_state=42)\n",
    "    clf.fit(X_train_comb, y_train)\n",
    "    y_pred = clf.predict(X_test_comb)\n",
    "    metrics = evaluate_metrics(y_test, y_pred)\n",
    "    bundle = {\"vect\": vect, \"svd\": svd, \"scaler\": scaler, \"clf\": clf}\n",
    "    return metrics, bundle\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--test_size\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--n_components\", type=int, nargs=\"+\", default=[50,100,200],\n",
    "                        help=\"List of n_components to try for TruncatedSVD (e.g. --n_components 50 100 200)\")\n",
    "    parser.add_argument(\"--save_best\", action=\"store_true\", help=\"Save best baseline and best hybrid bundles\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball',\n",
    "                  'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    X, y, target_names = load_data(categories)\n",
    "    print(f\"Samples: {len(X)}, classes: {len(target_names)}\")\n",
    "\n",
    "    # Single split used for fair comparison\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, stratify=y, random_state=42)\n",
    "\n",
    "    results = []\n",
    "    print(\"\\n--- Baseline (TF-IDF -> LR) ---\")\n",
    "    base_metrics, base_bundle = baseline_pipeline_train_eval(X_train, X_test, y_train, y_test)\n",
    "    print(f\"Baseline metrics (acc, prec, rec, f1): {base_metrics}\")\n",
    "    results.append({\n",
    "        \"model\": \"baseline_tfidf_lr\",\n",
    "        \"n_components\": None,\n",
    "        \"accuracy\": base_metrics[0],\n",
    "        \"precision\": base_metrics[1],\n",
    "        \"recall\": base_metrics[2],\n",
    "        \"f1\": base_metrics[3]\n",
    "    })\n",
    "\n",
    "    best_hybrid = None\n",
    "    best_hybrid_score = -1.0\n",
    "    hybrid_bundles = {}\n",
    "\n",
    "    for nc in args.n_components:\n",
    "        print(f\"\\n--- Hybrid (n_components={nc}) ---\")\n",
    "        metrics, bundle = hybrid_train_eval(X_train, X_test, y_train, y_test, n_components=nc)\n",
    "        print(f\"Hybrid (n={nc}) metrics (acc, prec, rec, f1): {metrics}\")\n",
    "        results.append({\n",
    "            \"model\": \"hybrid_tfidf_lsa_lr\",\n",
    "            \"n_components\": nc,\n",
    "            \"accuracy\": metrics[0],\n",
    "            \"precision\": metrics[1],\n",
    "            \"recall\": metrics[2],\n",
    "            \"f1\": metrics[3]\n",
    "        })\n",
    "        hybrid_bundles[nc] = bundle\n",
    "        # use accuracy (or f1) to pick best hybrid\n",
    "        if metrics[0] > best_hybrid_score:\n",
    "            best_hybrid_score = metrics[0]\n",
    "            best_hybrid = (nc, bundle, metrics)\n",
    "\n",
    "    # Show results as DataFrame sorted by accuracy\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values(by=\"accuracy\", ascending=False).reset_index(drop=True)\n",
    "    print(\"\\n\\n=== Comparison table (sorted by accuracy) ===\")\n",
    "    print(df.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "\n",
    "    # Optionally save best models\n",
    "    if args.save_best:\n",
    "        print(\"\\nSaving best models...\")\n",
    "        joblib.dump(base_bundle, \"best_baseline_news_tfidf_lr.joblib\")\n",
    "        if best_hybrid is not None:\n",
    "            nc_best, bundle_best, metrics_best = best_hybrid\n",
    "            joblib.dump(bundle_best, f\"best_hybrid_news_tfidf_lsa_lr_n{nc_best}.joblib\")\n",
    "            print(f\"Saved baseline and best hybrid (n_components={nc_best})\")\n",
    "        else:\n",
    "            print(\"No hybrid models found to save.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ed6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_df=0.9, min_df=3, ngram_range=(1, 2),\n",
      "                                 stop_words='english', sublinear_tf=True)),\n",
      "                ('clf',\n",
      "                 LogisticRegression(max_iter=2000, n_jobs=-1, random_state=42,\n",
      "                                    solver='saga'))])\n",
      "{'tfidf': TfidfVectorizer(max_df=0.9, min_df=3, ngram_range=(1, 2), stop_words='english',\n",
      "                sublinear_tf=True), 'clf': LogisticRegression(max_iter=2000, n_jobs=-1, random_state=42, solver='saga')}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load pipeline\n",
    "pipeline = joblib.load(\"news_tfidf_lr.joblib\")\n",
    "\n",
    "# See the steps inside the pipeline\n",
    "print(pipeline)\n",
    "print(pipeline.named_steps)  # dictionary of steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8023c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: [2]\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\"i play football\"]\n",
    "prediction = pipeline.predict(sample_text)\n",
    "print(\"Predicted category:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc22fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load dataset target names\n",
    "categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball',\n",
    "              'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "sample_text = [\"i play football\"]\n",
    "prediction = pipeline.predict(sample_text)\n",
    "print(\"Predicted category:\", categories[prediction[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d45487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: sci.electronics\n",
      "\n",
      "Category probabilities:\n",
      "comp.graphics: 0.1845\n",
      "comp.sys.ibm.pc.hardware: 0.1313\n",
      "comp.sys.mac.hardware: 0.1268\n",
      "sci.electronics: 0.2023\n",
      "rec.sport.baseball: 0.1649\n",
      "soc.religion.christian: 0.1074\n",
      "talk.politics.guns: 0.0827\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Choose categories (added more tech ones for better GPU detection)\n",
    "categories = [\n",
    "    'comp.graphics',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.sys.mac.hardware',\n",
    "    'sci.electronics',\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns'\n",
    "]\n",
    "\n",
    "# Load training data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "\n",
    "# Sample text to test\n",
    "sample_text = [\"messi plays football very well\"]\n",
    "\n",
    "# Prediction\n",
    "prediction = pipeline.predict(sample_text)[0]\n",
    "print(\"Predicted category:\", categories[prediction])\n",
    "\n",
    "# Show probabilities for better understanding\n",
    "probs = pipeline.predict_proba(sample_text)[0]\n",
    "print(\"\\nCategory probabilities:\")\n",
    "for cat, prob in zip(categories, probs):\n",
    "    print(f\"{cat}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as news_classifier.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Categories\n",
    "categories = [\n",
    "    'comp.graphics',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.sys.mac.hardware',\n",
    "    'sci.electronics',\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns'\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(newsgroups_train.data, newsgroups_train.target)\n",
    "\n",
    "# ✅ Save the trained model\n",
    "joblib.dump((pipeline, categories), \"news_classifier.joblib\")\n",
    "\n",
    "print(\"Model saved as news_classifier.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbc3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: comp.sys.ibm.pc.hardware\n",
      "\n",
      "Category probabilities:\n",
      "comp.graphics: 0.1836\n",
      "comp.sys.ibm.pc.hardware: 0.3434\n",
      "comp.sys.mac.hardware: 0.1596\n",
      "sci.electronics: 0.0650\n",
      "rec.sport.baseball: 0.1062\n",
      "soc.religion.christian: 0.0693\n",
      "talk.politics.guns: 0.0730\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load model + categories\n",
    "pipeline, categories = joblib.load(\"news_classifier.joblib\")\n",
    "\n",
    "# Test\n",
    "sample_text = [\"graphic card prices increased recently\"]\n",
    "prediction = pipeline.predict(sample_text)[0]\n",
    "\n",
    "print(\"Predicted category:\", categories[prediction])\n",
    "\n",
    "# Show probabilities\n",
    "probs = pipeline.predict_proba(sample_text)[0]\n",
    "print(\"\\nCategory probabilities:\")\n",
    "for cat, prob in zip(categories, probs):\n",
    "    print(f\"{cat}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336c79de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JOBLIB FILES ANALYSIS ===\n",
      "These files contain trained machine learning models and preprocessors\n",
      "They represent the 'processed dataset' in machine learning pipeline\n",
      "\n",
      "=== Examining news_tfidf_lr.joblib ===\n",
      "Type: <class 'sklearn.pipeline.Pipeline'>\n",
      "File size: 1707044 bytes\n",
      "\n",
      "=== Examining news_tfidf_lsa_lr.joblib ===\n",
      "Type: <class 'dict'>\n",
      "Contents:\n",
      "  tfidf_vect: <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "  svd: <class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>\n",
      "    - Components: 100\n",
      "    - Explained variance ratio: [0.00291051 0.00488337 0.00362288 0.00329944 0.00275583]...\n",
      "  scaler: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "    - Features scaled: 100\n",
      "  clf: <class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "    - Classes: 5\n",
      "    - Algorithm: saga\n",
      "  categories: <class 'list'>\n",
      "File size: 23251148 bytes\n",
      "File best_baseline_news_tfidf_lr.joblib not found\n",
      "File best_hybrid_news_tfidf_lsa_lr_n50.joblib not found\n",
      "File best_hybrid_news_tfidf_lsa_lr_n100.joblib not found\n",
      "File best_hybrid_news_tfidf_lsa_lr_n200.joblib not found\n",
      "\n",
      "=== SUMMARY ===\n",
      "The .joblib files you see in the project folder contain:\n",
      "1. Trained models that can make predictions on new text\n",
      "2. Fitted preprocessors (TF-IDF vectorizers) that convert text to numbers\n",
      "3. All the learned parameters from training on the 20 Newsgroups dataset\n",
      "4. These are the 'final products' of the machine learning pipeline\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to show what's inside the .joblib files for professor demonstration\n",
    "\"\"\"\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def examine_joblib_file(filepath):\n",
    "    \"\"\"Examine contents of a joblib file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} not found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Examining {filepath} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load the saved model bundle\n",
    "        bundle = joblib.load(filepath)\n",
    "        \n",
    "        print(f\"Type: {type(bundle)}\")\n",
    "        \n",
    "        if isinstance(bundle, dict):\n",
    "            print(\"Contents:\")\n",
    "            for key, value in bundle.items():\n",
    "                print(f\"  {key}: {type(value)}\")\n",
    "                \n",
    "                # Show more details for specific components\n",
    "                if key == 'vect':  # TF-IDF Vectorizer\n",
    "                    print(f\"    - Vocabulary size: {len(value.vocabulary_)} words\")\n",
    "                    print(f\"    - N-gram range: {value.ngram_range}\")\n",
    "                    print(f\"    - Max features: {value.max_features}\")\n",
    "                    \n",
    "                elif key == 'svd':  # TruncatedSVD (LSA)\n",
    "                    print(f\"    - Components: {value.n_components}\")\n",
    "                    print(f\"    - Explained variance ratio: {value.explained_variance_ratio_[:5]}...\")\n",
    "                    \n",
    "                elif key == 'clf':  # Classifier\n",
    "                    print(f\"    - Classes: {len(value.classes_)}\")\n",
    "                    print(f\"    - Algorithm: {value.solver}\")\n",
    "                    \n",
    "                elif key == 'scaler':  # StandardScaler\n",
    "                    print(f\"    - Features scaled: {len(value.mean_)}\")\n",
    "        \n",
    "        print(f\"File size: {os.path.getsize(filepath)} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "\n",
    "# Check if the files exist and examine them\n",
    "files_to_check = [\n",
    "    'news_tfidf_lr.joblib',\n",
    "    'news_tfidf_lsa_lr.joblib',\n",
    "    'best_baseline_news_tfidf_lr.joblib',\n",
    "    'best_hybrid_news_tfidf_lsa_lr_n50.joblib',\n",
    "    'best_hybrid_news_tfidf_lsa_lr_n100.joblib',\n",
    "    'best_hybrid_news_tfidf_lsa_lr_n200.joblib'\n",
    "]\n",
    "\n",
    "print(\"=== JOBLIB FILES ANALYSIS ===\")\n",
    "print(\"These files contain trained machine learning models and preprocessors\")\n",
    "print(\"They represent the 'processed dataset' in machine learning pipeline\")\n",
    "\n",
    "for filepath in files_to_check:\n",
    "    examine_joblib_file(filepath)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"The .joblib files you see in the project folder contain:\")\n",
    "print(\"1. Trained models that can make predictions on new text\")\n",
    "print(\"2. Fitted preprocessors (TF-IDF vectorizers) that convert text to numbers\")\n",
    "print(\"3. All the learned parameters from training on the 20 Newsgroups dataset\")\n",
    "print(\"4. These are the 'final products' of the machine learning pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
